---
title: 'Session 10: Data Science Capstone Project'
author: "Alexandru-Victor Botorog"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: haddock
      theme: spacelab
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
div.navy1 { background-color:#686868; border-radius: 5px; padding: 20px; border-style: groove; color: #ffffff;}

</style>



```{r, load_libraries, include = FALSE}

if(!is.element("tidyverse", installed.packages()[,1]))
{  install.packages("tidyverse", repos = "http://cran.us.r-project.org")}

if(!is.element("Hmisc", installed.packages()[,1]))
{  install.packages("Hmisc", repos = "http://cran.us.r-project.org")} #package for data summary using `describe`

if(!is.element("ggplot2", installed.packages()[,1]))
{  install.packages("ggplot2", repos = "http://cran.us.r-project.org")} #package for plots
if(!is.element("ggthemes", installed.packages()[,1]))
{  install.packages("ggthemes", repos = "http://cran.us.r-project.org")} #package to make fancier ggplots

if(!is.element("janitor", installed.packages()[,1]))
{ install.packages("janitor", repos = "http://cran.us.r-project.org")} #package to visualize results of machine learning tools
if(!is.element("rpart.plot", installed.packages()[,1]))
{  install.packages("rpart.plot", repos = "http://cran.us.r-project.org")} #package to visualize trees

library(rpart.plot)
library(caret)
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate)
library(janitor) # clean_names()
library(Hmisc)
library(skimr)
```

# Introduction and learning objectives

<div class = "navy1">
The purpose of this exercise is to build an estimation engine to guide investment decisions in London house market. You will first build machine learning algorithms (and tune them) to estimate the house prices given variety of information about each property. Then, using your algorithm, you will choose 200 houses to invest in out of about 2000 houses on the market at the moment.


<b>Learning objectives</b>
 
<ol type="i">
  <li>Using different data mining algorithms for prediction.</li>
  <li>Dealing with large data sets</li>
  <li>Tuning data mining algorithms</li>
  <li>Interpreting data mining algorithms and deducing importance of variables</li>
  <li>Using results of data mining algorithms to make business decisions</li>
</ol>  
</div>

# Load data

There are two sets of data, i) training data that has the actual prices ii) out of sample data that has the asking prices. Load both data sets. 

Make sure you understand what information each column contains. Note that not all information provided might be useful in predicting house prices, but do not make any assumptions before you decide what information you use in your prediction algorithms.

```{r read-investigate}
#read in the data

london_house_prices_2019_training<-read.csv("training_data_assignment_with_prices.csv")
london_house_prices_2019_out_of_sample<-read.csv("test_data_assignment.csv")

#fix data types in both data sets

#fix dates
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate(date=as.Date(date))
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate(date=as.Date(date))
#change characters to factors
london_house_prices_2019_training <- london_house_prices_2019_training %>% mutate_if(is.character,as.factor)
london_house_prices_2019_out_of_sample<-london_house_prices_2019_out_of_sample %>% mutate_if(is.character,as.factor)

#take a quick look at what's in the data
glimpse(london_house_prices_2019_training)
glimpse(london_house_prices_2019_out_of_sample)
describe(london_house_prices_2019_training)
describe(london_house_prices_2019_out_of_sample)
```


```{r split the price data to training and testing}
#let's do the initial split
library(rsample)
set.seed(1234)
train_test_split <- initial_split(london_house_prices_2019_training, prop = 0.75) #training set contains 75% of the data
# Create the training dataset
train_data <- training(train_test_split)
test_data <- testing(train_test_split)
```


# Visualize data 

Visualize and examine the data. What plots could be useful here? What do you learn from these visualizations?
> For examining the relationship between price and categorical variables, we will mainly use boxplots and
bar charts. For examining the relationship between price and continuous numerical variables,
we will use scatter plots with trend lines. These visualizations will give us an initial overview 
of the most important factors that determine house prices. I will also look at the distribution 
of prices using a density plot to see if there are outliers. Moreover, even though the testing
dataset doesn't have data on purchase date, I will also examine the time trend of median house
prices to see if there are any significant adjustments.

```{r visualize}
#Time trend of the median house price
london_house_prices_2019_training %>%
  mutate(ym=format_ISO8601(date,  precision = "ym"))%>%
  group_by(ym)%>%
  mutate(ym=as.Date(paste0(ym,"-01")))%>%
  summarise(mean_price=mean(price))%>%
  ggplot(aes(x=ym, y=mean_price))+geom_point()+geom_line()
#We can see an overall upward trend for median house prices, with a significant
#increase occurring in July 2019

#===============================================================================

#Density plot of price
ggplot(london_house_prices_2019_training, aes(x=price)) + geom_density()
#clearly many outliers present, so best to log the price data

#Density plot of log price
ggplot(london_house_prices_2019_training, aes(x=log(price))) + geom_density()

#===============================================================================

#Relationship with categorical variables (will consider London Zone
#and number of lines to be categorical)

#median price by district
london_house_prices_2019_training%>%
  group_by(district)%>%
  summarise(med_price=median(price))%>%
  mutate(district=fct_reorder(district, med_price))%>%
  ggplot(aes(x=med_price, y=district))+geom_col()

#boxplot of log price by london zone
london_house_prices_2019_training %>%
  ggplot(aes(x=factor(london_zone),
             y=log(price),
             fill=factor(london_zone))) +
  geom_boxplot()

#boxplot of log price by property type
london_house_prices_2019_training %>%
  ggplot(aes(x=property_type,
             y=log(price),
             fill=property_type)) +
  geom_boxplot()

#boxplot of log price by freehold_or_leasehold
london_house_prices_2019_training %>%
  ggplot(aes(x=freehold_or_leasehold,
             y=log(price),
             fill=freehold_or_leasehold)) +
  geom_boxplot()

#boxplot of log price by old or new
london_house_prices_2019_training %>%
  ggplot(aes(x=whether_old_or_new,
             y=log(price),
             fill=whether_old_or_new)) +
  geom_boxplot()

#median price by energy rating faceted by property type 
london_house_prices_2019_training %>%
  group_by(current_energy_rating, property_type) %>%
  summarise(median_price = median(price)) %>%
  ggplot(aes(x=current_energy_rating,
             y=median_price,
             fill=property_type)) +
  geom_col()+facet_wrap(~property_type)

#boxplot of log price by water company
london_house_prices_2019_training %>%
  ggplot(aes(x=water_company,
             y=log(price),
             fill=water_company)) +
  geom_boxplot()

#boxplot of log price by tenure
london_house_prices_2019_training %>%
  ggplot(aes(x=tenure,
             y=log(price),
             fill=tenure)) +
  geom_boxplot()

#boxplot of log price by type of closest station faceted by london zone
london_house_prices_2019_training %>%
  filter(london_zone!=7)%>%
  ggplot(aes(x=type_of_closest_station,
             y=log(price),
             fill=type_of_closest_station)) +
  geom_boxplot()+facet_wrap(~london_zone)

#boxplot of log price by number of tube lines
london_house_prices_2019_training %>%
  ggplot(aes(x=factor(num_tube_lines),
             y=log(price),
             fill=factor(num_tube_lines))) +
  geom_boxplot()

#all categorical variables examined above seem to have some effect on price

#===============================================================================

#Relationship with numerical variables

#distance to station
ggplot(london_house_prices_2019_training, aes(x=distance_to_station, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)


#total floor area
ggplot(london_house_prices_2019_training, aes(x=total_floor_area, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

#average income
ggplot(london_house_prices_2019_training, aes(x=average_income, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

#population
ggplot(london_house_prices_2019_training, aes(x=population, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

#current CO2 emissions
ggplot(london_house_prices_2019_training, aes(x=co2_emissions_potential, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

#current energy consumption
ggplot(london_house_prices_2019_training, aes(x=energy_consumption_potential, y=log(price))) +
  geom_point() + geom_smooth(method = 'lm', se = TRUE)

#again, all continuous variables examined above seem to have some effect on price
```

Estimate a correlation table between prices and other continuous variables. What do you glean from the correlation table?


```{r, correlation table, warning=FALSE, message=FALSE}

# produce a correlation table using GGally::ggcor()
# this takes a while to plot

library("GGally")
london_house_prices_2019_training %>% 
  select(-ID) %>% #keep Y variable last
  ggcorr(method = c("pairwise", "pearson"), layout.exp = 2,label_round=2, label = TRUE,label_size = 2,hjust = 1,nbreaks = 5,size = 2,angle = -20)

```

>The correlation table above suggests that total floor area has the highest 
correlation with price. Number of habitable rooms is highly correlated with
total floor area, which is why it is also correlated with price. For this
reason it is probably best to exclude  number of habitable rooms from our
predictive models. CO2 emissions (current and potential) also have a strong 
correlation with price but also with each other. Because potential CO2
emissions are much less correlated with total floor area, I will only
use this variable in my predictive models. Neither potential nor current
energy consumption seem to be very correlated with price, and in my 
analysis below I will only use energy consumption potential because again
the two measures are quite correlated. Other variables that seem to be strongly
correlated with price are london zone, average income, and number of tube lines.
Moreover, most of the correlations above seem to have the right sign (+ for
floor area, CO2 emissions, average income, number of tube lines, - for energy
consumption, population, london zone, distance to station).


# Fit a linear regression model

To help you get started I build a linear regression model below. I chose a subset of the features with no particular goal. You can (and should) add more variables and/or choose variable selection methods if you want.

>Before getting started with the predictive models, I will mention which variables 
I have excluded from all the models. Firstly, I have not used any variables that 
the out of sample dataset doesn't have data on (e.g. date), or any categorical/ 
factor variable that contains more than 50 categories (e.g. postcode_short or 
nearest station) as this might make the models too computationally expensive. 
I have also excluded number of habitable rooms, current CO2  emissions and 
current energy consumption due to correlation with other predictors (discussed 
above), as well as windows energy efficiency since it is a worse version of 
current energy rating. I will also use log price as the dependent variable 
throughout the entire project.


```{r LR model, fig.height = 8, fig.width = 8}
set.seed(1234)
#Define control variables
control <- trainControl (
    method="cv",
    number=5,
    savePredictions = "final",
    verboseIter=FALSE)

#we are going to train the model and report the results using k-fold cross validation

#The features of my first linear regression model have been chosen through the
#methodology discussed above. Latitude and longitude are also excluded from the 
#regression model in favor of district and london zone for geographical price 
#determinants. Also, to be able to include population as a feature, I have
#only kept the observations from the training data that have data for this
#variable (which is most of them)

model1_lm<-train(
    log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential +co2_emissions_current+ energy_consumption_potential + energy_consumption_current+tenure + population + altitude + factor(london_zone) + water_company + average_income + district + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station,
    train_data%>%filter(is.na(population)==FALSE),
    method = "lm",
    trControl = control)
summary(model1_lm)
#RMSE=0.2265, Adjusted R-squared=0.8277, num_light_rail_lines only non-significant predictor


#checking for multicolinearity
library(car)
vif(lm( log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential +co2_emissions_current+ energy_consumption_potential + energy_consumption_current+tenure + population + altitude + factor(london_zone) + water_company + average_income + district + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station,
    train_data%>%filter(is.na(population)==FALSE)))
#no significant collinearity detected by vif


#variable importance
importance <- varImp(model1_lm, scale=TRUE)
plot(importance)


#remove num_light_rail_lines (insignificant) and add interactions between total_floor_area, the
#most important numerical variable, and the most important categorical variables
model2_lm<-train(
    log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential +co2_emissions_current+ energy_consumption_potential + energy_consumption_current+ tenure + population + altitude + factor(london_zone) + water_company + average_income + district + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + property_type:total_floor_area + total_floor_area:factor(london_zone) + total_floor_area:current_energy_rating + total_floor_area:whether_old_or_new + total_floor_area:type_of_closest_station + total_floor_area:freehold_or_leasehold,
    train_data%>%filter(is.na(population)==FALSE),
    method = "lm",
    trControl = control)
summary(model2_lm)
v #RMSE=0.2168, Adjusted R-squared=0.8421, all added interactions significant


#add second-order polynomials for continuous variables. I will not go beyond second-order polynomials
#due to lack of economic interpretation. I do not add a second-order polynomial 
#for total_floor_area because this generates a warning that prediction from a rank-deficient 
#fit may be misleading (also altitude, population, and distance to station second-order
#polynomials are not significant)

model3_lm<-train(
    log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + poly(co2_emissions_potential,2) + poly(energy_consumption_potential,2) + tenure + population + altitude + factor(london_zone) + water_company + poly(average_income,2) + district + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + property_type:total_floor_area + total_floor_area:factor(london_zone) + total_floor_area:current_energy_rating + total_floor_area:whether_old_or_new + total_floor_area:type_of_closest_station + total_floor_area:freehold_or_leasehold,
    train_data%>%filter(is.na(population)==FALSE),
    method = "lm",
    trControl = control)
summary(model3_lm)
#RMSE=0.215, Adjusted R-squared=0.8447, all added polynomial significant
```


```{r importance final lm, fig.height = 10, fig.width = 10}
# we can check variable importance as well
importance <- varImp(model3_lm, scale=TRUE)
ggplot(importance) 
```

## Predict the values in testing and out of sample data

Below I use the predict function to test the performance of the model in testing data and summarize the performance of the linear regression model. How can you measure the quality of your predictions?

>Quality of predictions is measured by out of sample RMSE and R2.

```{r pred lm}
# We can predict the testing values
test_data_lm<-test_data%>%filter(is.na(population)==FALSE)

predictions_lm <- predict(model2_lm,test_data_lm)

lm_results<-data.frame(Method="Lm",RMSE = RMSE(predictions_lm, log(test_data_lm$price)), 
                        Rsquare = R2(predictions_lm, log(test_data_lm$price)))

                            
lm_results 

#We can predict prices for out of sample data the same way
predictions_oos_lm <- predict(model3_lm,london_house_prices_2019_out_of_sample)
```

# Fit a tree model

Next I fit a tree model using the same subset of features. Again you can (and should) add more variables and tune the parameter of your tree to find a better fit. 

Compare the performance of the linear regression model with the tree model; which one performs better? Why do you think that is the case?

```{r tree model, fig.height = 7, fig.width = 7}
#For the tree model, we will use the same features as for the first lm model (the 
#one without any polynomials and interactions), and replacing district with 
#latitude and longitude, as these work much better with tree-based models for 
#determining geographical price effects.

#we will tune the cp parameter to pick the optimal tree model
set.seed(1234)
model1_tree <- train(
  log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
  train_data%>%filter(is.na(population)==FALSE),
  method = "rpart",
  trControl = control,
  metric="RMSE",
  tuneLength=10)
print(model1_tree)
plot(model1_tree)
#clearly the optimal cp is below 0.01


Grid <- expand.grid(cp = seq(0, 0.01, by=0.0001))

set.seed(1234)
model2_tree <- train(
  log(price) ~ property_type+whether_old_or_new+freehold_or_leasehold+current_energy_rating+total_floor_area+ co2_emissions_potential+energy_consumption_potential+tenure+population+altitude+factor(london_zone)+water_company+average_income+latitude+longitude+type_of_closest_station+num_tube_lines+num_rail_lines+num_light_rail_lines+distance_to_station,
  train_data%>%filter(is.na(population)==FALSE),
  method = "rpart",
  trControl = control,
  metric="RMSE",
  tuneGrid=Grid)
plot(model2_tree)
print(model2_tree)
#model2_tree: RMSE=0.2637303, R2=0.7678223, cp=0.0003

#visualizing variable importance
importance <- varImp(model2_tree, scale=TRUE)
plot(importance)
#latitude and longitude are clearly very important for tree-based models
```


```{r pred tree}
# We can predict the testing values
test_data_tree<-test_data%>%filter(is.na(population)==FALSE)

predictions_tree <- predict(model2_tree,test_data_tree)

tree_results<-data.frame(Method="Trees", RMSE = RMSE(predictions_tree, log(test_data_tree$price)), 
                            Rsquare = R2(predictions_tree, log(test_data_tree$price)))

                            
tree_results     
    

#We can predict prices for out of sample data the same way
predictions_oos_tree <- predict(model2_tree,london_house_prices_2019_out_of_sample)
```
# Other algorithms

Use at least two other algorithms to predict prices. Don't forget to tune the parameters of these algorithms. And then compare the performances of your algorithms to linear regression and trees.

```{r knn model}

#For knn, we only keep numerical variables, categorical variables with only 2 
#categories as well as categorical variables with more than 2 categories that can 
#be nonetheless converted to a numerical scale
train_data_knn<-train_data%>%select(price, total_floor_area, current_energy_rating, number_habitable_rooms, co2_emissions_potential, energy_consumption_potential, latitude, longitude, population, altitude, london_zone, average_income, num_tube_lines, num_light_rail_lines, num_rail_lines, distance_to_station, property_type, whether_old_or_new, freehold_or_leasehold, tenure)%>%
                                  filter(is.na(population)==FALSE)%>%
                                   mutate(current_energy_rating=case_when( current_energy_rating=="B"~1,current_energy_rating=="C"~2,current_energy_rating=="D"~3,current_energy_rating=="E"~4,current_energy_rating=="F"~5,current_energy_rating=="G"~6),property_type=case_when(property_type=="D"~1, property_type=="S"~2,property_type=="T"~3, property_type=="F"~4), tenure=case_when(tenure=="owner-occupied"~1, tenure=="rental (private)"~2, tenure=="rental (social)"~3), whether_old_or_new=case_when(whether_old_or_new=="Y"~1, whether_old_or_new=="N"~2), freehold_or_leasehold=case_when(freehold_or_leasehold=="F"~1, freehold_or_leasehold=="L"~2))


#we will tune the k parameter to pick the optimal knn model
set.seed(1234)
model1_knn <- train(log(price)~., data=train_data_knn, 
                 method = "knn",
                 trControl = control,
                 measure="RMSE",
                 preProcess = c("center", "scale"),
                 tuneLength = 10)
plot(model1_knn)
print(model1_knn)

#searching in the vicinity of k=13
knnGrid <-  expand.grid(k= seq(9, 17 , by = 1)) 
set.seed(1234)
model2_knn <- train(log(price)~., data=train_data_knn, 
                 method = "knn",
                 trControl = control,
                 measure="RMSE",
                 preProcess = c("center", "scale"),
                 tuneGrid = knnGrid)
plot(model2_knn)
print(model2_knn)
#model2_knn: RMSE=0.2575751, R2=0.7860665, k=13

importance <- varImp(model2_knn, scale=TRUE)
plot(importance)
```


```{r pred knn}
# We can predict the testing values
test_data_knn<-test_data%>%select(total_floor_area, current_energy_rating, number_habitable_rooms, co2_emissions_potential, energy_consumption_potential, latitude, longitude, population, altitude, london_zone, average_income, price, num_tube_lines, num_light_rail_lines, num_rail_lines, distance_to_station, property_type, whether_old_or_new, freehold_or_leasehold, tenure)%>%
                                  filter(is.na(population)==FALSE)%>%
                                   mutate(current_energy_rating=case_when( current_energy_rating=="B"~1,current_energy_rating=="C"~2,current_energy_rating=="D"~3,current_energy_rating=="E"~4,current_energy_rating=="F"~5,current_energy_rating=="G"~6),property_type=case_when(property_type=="D"~1, property_type=="S"~2,property_type=="T"~3, property_type=="F"~4), tenure=case_when(tenure=="owner-occupied"~1, tenure=="rental (private)"~2, tenure=="rental (social)"~3), whether_old_or_new=case_when(whether_old_or_new=="Y"~1, whether_old_or_new=="N"~2), freehold_or_leasehold=case_when(freehold_or_leasehold=="F"~1, freehold_or_leasehold=="L"~2))


predictions_knn <- predict(model2_knn,test_data_knn)

knn_results<-data.frame(Method="KNN", RMSE = RMSE(predictions_knn, log(test_data_knn$price)), 
                            Rsquare = R2(predictions_knn, log(test_data_knn$price)))

                            
knn_results     
    

#We can predict prices for out of sample data the same way
predictions_oos_knn <- predict(model2_knn,london_house_prices_2019_out_of_sample%>%
                                   mutate(current_energy_rating=case_when( current_energy_rating=="B"~1,current_energy_rating=="C"~2,current_energy_rating=="D"~3,current_energy_rating=="E"~4,current_energy_rating=="F"~5,current_energy_rating=="G"~6),property_type=case_when(property_type=="D"~1, property_type=="S"~2,property_type=="T"~3, property_type=="F"~4), tenure=case_when(tenure=="owner-occupied"~1, tenure=="rental (private)"~2, tenure=="rental (social)"~3), whether_old_or_new=case_when(whether_old_or_new=="Y"~1, whether_old_or_new=="N"~2), freehold_or_leasehold=case_when(freehold_or_leasehold=="F"~1, freehold_or_leasehold=="L"~2)))
```


```{r random forrests model, fig.height = 7, fig.width = 7}
#since random forests is a tree-based ensemble method, we will use the same features
#as in the tree model

#looking for best mtry for min node size = 10
set.seed(1234)
model1_rf <- train(
  log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
  train_data%>%filter(is.na(population)==FALSE),
  method = "ranger",
  trControl = control,
  metric="RMSE",
  tuneGrid=data.frame(.mtry = c(2:20), .splitrule = "variance", .min.node.size = 10))
print(model1_rf)
plot(model1_rf)
#best mtry chosen as 11 


#looking for best min.node.size for mtry = 11
set.seed(1234)
model2_rf <- train(
  log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
  train_data%>%filter(is.na(population)==FALSE),
  method = "ranger",
  trControl = control,
  metric="RMSE",
  tuneGrid=data.frame(.mtry = 11, .splitrule = "variance", .min.node.size = c(2:15)))
print(model2_rf)
plot(model2_rf)
#best min node size chosen as 2


#checking if other splitrules deliver better results than the default (variance)
set.seed(1234)
model3_rf<-train(
  log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
  train_data%>%filter(is.na(population)==FALSE),
  method = "ranger",
  trControl = control,
  metric="RMSE",
  tuneGrid=data.frame(.mtry = 11, .splitrule = c("variance", "extratrees", "maxstat"), .min.node.size = 2),
  importance = 'permutation')
print(model3_rf)
#variance seems to be the splitrule that delivers the best result
#model3_rf: RMSE=0.2150983, R2=0.8470335, splitrule=variance, mtry=11, min.node.size=2

varImp(model3_rf)
plot(varImp(model3_rf))
```
```{r pred rf}
# We can predict the testing values
test_data_rf<-test_data%>%filter(is.na(population)==FALSE)

predictions_rf<- predict(model3_rf,test_data_rf)

rf_results<-data.frame(Method="RF", RMSE = RMSE(predictions_rf, log(test_data_rf$price)), 
                            Rsquare = R2(predictions_rf, log(test_data_rf$price)))

                            
rf_results     
    

#We can predict prices for out of sample data the same way
predictions_oos_rf <- predict(model3_rf,london_house_prices_2019_out_of_sample)
```



```{r gbm}
#since gbm is a tree-based ensemble method, we will use the same features
#as in the tree model

#looking for the best interaction.depth for n.trees=100, shrinkage =0.075, 
#n.minobsinnode = 10
grid<-expand.grid(interaction.depth = seq(5, 45, by=5), n.trees = 100, shrinkage =0.075, n.minobsinnode = 10)
set.seed(1234)
model1_gbm <- train(
            log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
            train_data%>%filter(is.na(population)==FALSE),
            method = "gbm", 
            trControl = control,
            tuneGrid =grid,
            metric = "RMSE",
            verbose = FALSE)
print(model1_gbm)
plot(model1_gbm)
#we can see that the performance improves little once we pass interaction.depth=35.
#Therefore, we will proceed with interaction.depth=45 as it delivers the lowest
#RMSE and going beyond interaction.depth=50 isn't allowed.


#looking at the impact of n.trees on RMSE performance for interaction.depth=45, 
#shrinkage = 0.075, n.minobsinnode = 10
grid<-expand.grid(interaction.depth = 45, n.trees=seq(60, 140, by=10), shrinkage=0.075, n.minobsinnode = 10)
set.seed(1234)
model2_gbm <- train(
            log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
            train_data%>%filter(is.na(population)==FALSE),
            method = "gbm", 
            trControl = control,
            tuneGrid =grid,
            metric = "RMSE",
            verbose = FALSE)
print(model2_gbm)
plot(model2_gbm)
#Increasing n.trees improves model performance, but not significantly after 
#n.trees=120. We will therefore stick with n.trees=140 and not go beyond.


#looking at the impact of shrinkage on RMSE performance for interaction.depth=45, 
#n.trees=140, n.minobsinnode = 10
grid<-expand.grid(interaction.depth = 45, n.trees = 140 , shrinkage = seq(0.045, 0.105, by=0.01), n.minobsinnode = 10)
set.seed(1234)
model3_gbm <- train(
            log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
            train_data%>%filter(is.na(population)==FALSE),
            method = "gbm", 
            trControl = control,
            tuneGrid =grid,
            metric = "RMSE",
            verbose = FALSE)
plot(model3_gbm)
print(model3_gbm)
#shrinkage seems to deliver roughly constant performance between 0.055 and 0.085.
#Therefore, we will stick with shrinkage=0.065 since it delivered the best performance
#here


#looking at the impact of n.minobsinnode on RMSE performance for interaction.depth=45, 
#n.trees=140, shrinkage= 0.065
grid<-expand.grid(interaction.depth = 45, n.trees = 140 , shrinkage = 0.065, n.minobsinnode=seq(5, 50, by=5))
set.seed(1234)
model4_gbm <- train(
            log(price) ~ property_type + whether_old_or_new + freehold_or_leasehold + current_energy_rating + total_floor_area + co2_emissions_potential + energy_consumption_potential + tenure + population + altitude + factor(london_zone) + water_company + average_income + type_of_closest_station + num_tube_lines + num_rail_lines + num_light_rail_lines + distance_to_station + latitude + longitude,
            train_data%>%filter(is.na(population)==FALSE),
            method = "gbm", 
            trControl = control,
            tuneGrid =grid,
            metric = "RMSE",
            verbose = FALSE)
plot(model4_gbm)
print(model4_gbm)
#Performance seems to be the best for n.minobsinnode between 5 and 20. We will 
#stick with it the original value of 10 as it delivers the best performance

#model4_gbm: RMSE=0.2057483, R2=0.8577135, interaction.depth = 45, n.trees = 140,
#shrinkage = 0.065, n.minobsinnode=10

varImp(model4_gbm)
plot(varImp(model4_gbm))
```
```{r pred gbm}
# We can predict the testing values
test_data_gbm<-test_data%>%filter(is.na(population)==FALSE)

predictions_gbm<- predict(model4_gbm,test_data_gbm)

gbm_results<-data.frame(Method="GBM", RMSE = RMSE(predictions_gbm, log(test_data_gbm$price)), 
                            Rsquare = R2(predictions_gbm, log(test_data_gbm$price)))

                            
gbm_results     
  
models_compare=rbind(lm_results,tree_results,knn_results,rf_results,gbm_results)
models_compare
#GBM and RF perform the best, followed by LM

#We can predict prices for out of sample data the same way
predictions_oos_gbm <- predict(model4_gbm,london_house_prices_2019_out_of_sample)
```


# Stacking

Use stacking to ensemble your algorithms.

```{r stacking}
#for the stacking part we will only use the 3 models that perform the best (lm, rf,
#and gbm), since they perform far better than the other 2 (knn, tree). However,
#in order to stack these three methods, we need to choose the same features for
#all 3 models (couldn't find how to stack models with different features
#unfortunately). Therefore, we will pick the set of features we used for both 
#gbm and rf, but also add district as this variable significantly improves the 
#lm algorithm. For rf we will chose the hyperparameters identified as best 
#beforehand (mtry=11, splitrule="variance", min.node-size=2). We do the same for 
#GBM (interaction.depth = 45, n.trees = 140, shrinkage = 0.065, n.minobsinnode=10). 
#Moreover, we need to remove population unfortunately since this variable isn't
#specified for all observations in the out of sample dataset.

library("caretEnsemble")
set.seed(1234)
model_list <- caretList(
    log(price) ~ property_type+whether_old_or_new+freehold_or_leasehold+current_energy_rating+total_floor_area+ co2_emissions_potential+energy_consumption_potential+tenure+altitude+factor(london_zone)+water_company+average_income+type_of_closest_station+num_tube_lines+num_rail_lines+num_light_rail_lines+distance_to_station+latitude+longitude+district,
    train_data,
    trControl=control,
    metric = "RMSE",
    methodList=c("lm"),
    tuneList=list(gbm=caretModelSpec(method="gbm", tuneGrid=data.frame(interaction.depth = 45,n.trees = 140, shrinkage=0.065, n.minobsinnode = 10),verbose = FALSE),
              ranger=caretModelSpec(method="ranger", tuneGrid=data.frame(mtry=11,splitrule="variance", min.node.size=2))))
  

summary(model_list)  
modelCor(resamples(model_list))
resamples <- resamples(model_list)
dotplot(resamples, metric = "RMSE")
#we can see that the performance of lm deteriorates significantly compared to
#the final lm model we picked


lm_ensemble <- caretStack(
    model_list, #Models we trained above in caretList 
    method="lm", #Use linear regression to combine
    metric="RMSE", #Use RMSE to as measure of fit quality
    trControl=control)
  
summary(lm_ensemble)
#RMSE=0.2036, Adj. R2=0.8609
```

```{r pred stack}
# We can predict the testing values
test_data_stack<-test_data

predictions_stack<- predict(lm_ensemble,test_data_stack)

stack_results<-data.frame(Method="Stack", RMSE = RMSE(predictions_stack, log(test_data_stack$price)), 
                            Rsquare = R2(predictions_stack, log(test_data_stack$price)))

                            
stack_results     
  
models_compare_stack<-rbind(models_compare,stack_results)
models_compare_stack
#We can see that stacking performs better than any other individual model out
#of sample

#We can predict prices for out of sample data the same way
predictions_oos_stack <- predict(lm_ensemble,london_house_prices_2019_out_of_sample)
```

# Pick investments

In this section you should use the best algorithm you identified to choose 200 properties from the out of sample data.

```{r pick investments}
#let's first train the final model on the entire training dataset since this might give us more
#reliable predictions
set.seed(1234)
model_list_all_data <- caretList(
    log(price) ~ property_type+whether_old_or_new+freehold_or_leasehold+current_energy_rating+total_floor_area+ co2_emissions_potential+energy_consumption_potential+tenure+altitude+factor(london_zone)+water_company+average_income+type_of_closest_station+num_tube_lines+num_rail_lines+num_light_rail_lines+distance_to_station+latitude+longitude+district,
    london_house_prices_2019_training,
    trControl=control,
    metric = "RMSE",
    methodList=c("lm"),
    tuneList=list(gbm=caretModelSpec(method="gbm", tuneGrid=data.frame(interaction.depth = 45,n.trees = 140, shrinkage=0.065, n.minobsinnode = 10),verbose = FALSE),
              ranger=caretModelSpec(method="ranger", tuneGrid=data.frame(mtry=11,splitrule="variance", min.node.size=2))))

lm_ensemble_all_data <- caretStack(
    model_list, #Models we trained above in caretList 
    method="lm", #Use linear regression to combine
    metric="RMSE", #Use RMSE to as measure of fit quality
    trControl=control)

predictions_oos_stack_all_data <- predict(lm_ensemble_all_data,london_house_prices_2019_out_of_sample)

#Now we simply attach a column with the prices predicted by the stacking algorithm
#to the oos dataset. Then we calculate the predicted returns and create a new column
#called "buy" which takes the value 1 if the predicted return of the observation is
#in the top 200 predicted returns


london_house_prices_2019_out_of_sample_with_pred<-london_house_prices_2019_out_of_sample%>%
  mutate(pred_prices=exp(predictions_oos_stack_all_data), return=(pred_prices-asking_price)/asking_price)

london_house_prices_2019_out_of_sample_with_pred_2<-london_house_prices_2019_out_of_sample_with_pred%>%
  mutate(buy=ifelse(return>sort(london_house_prices_2019_out_of_sample_with_pred$return, decreasing = TRUE)[[201]], 1, 0))

#calculate mean return
london_house_prices_2019_out_of_sample_with_pred_2%>%
  filter(buy==1)%>%
  summarise(mean_ret=mean(return))

#output your choices. Change the name of the file to your "lastname_firstname.csv"
write.csv(london_house_prices_2019_out_of_sample_with_pred_2,"Alexandru_Botorog.csv")
```
